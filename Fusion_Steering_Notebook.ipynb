{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7-5X3PJgR5-"
   },
   "source": [
    "# **Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U bitsandbytes datasets optuna matplotlib nltk mistralai transformers huggingface_hub numpy torch pandas seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistralai import Mistral\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline, GPT2LMHeadModel, GPT2TokenizerFast\n",
    "from huggingface_hub import hf_hub_download, notebook_login, login\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import os\n",
    "import optuna\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TORCHDYNAMO_DISABLE\"] = \"1\"\n",
    "\n",
    "#login(token=\"your_huggingface_api_token\")  # Replace with your actual API token\n",
    "\n",
    "quant_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "model_id = \"google/gemma-2-2b-it\"\n",
    "\n",
    "torch.set_grad_enabled(False) # avoid blowing up mem\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map='auto',\n",
    "    quantization_config=quant_config\n",
    ")\n",
    "\n",
    "torch.cuda.empty_cache()  # Frees up unused memory that has been allocated by PyTorch\n",
    "\n",
    "tokenizer =  AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mVpJq-sf2Rtb"
   },
   "source": [
    "# **Initial Inference: Gemma-2-2B-IT on SimpleQA**\n",
    "Generate base responses from the instruction-tuned Gemma-2-2B-IT model for the first 500 SimpleQA prompts. These outputs serve as the foundation for filtering, enrichment, and steering.\n",
    "\n",
    "**Note:** This step may take some time. Precomputed outputs are already included and used for evaluation, so re-execution is not necessary. This cell was intentionally not run to save time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SimpleQA dataset (top 500 rows)\n",
    "dataset = load_dataset(\"basicv8vc/SimpleQA\", split=\"test[:500]\")\n",
    "simpleqa_df = pd.DataFrame(dataset)  # Columns: metadata, problem, answer\n",
    "\n",
    "# Output container\n",
    "results = []\n",
    "\n",
    "for idx, row in simpleqa_df.iterrows():\n",
    "    question = row[\"problem\"]\n",
    "    ground_truth = row[\"answer\"]\n",
    "\n",
    "    prompt = f\"Question: {question}\\nAnswer:\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(input_ids=input_ids, max_new_tokens=100, temperature=0.0)\n",
    "        response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    results.append({\n",
    "        \"prompt\": question,\n",
    "        \"response\": response,\n",
    "        \"ground_truth\": ground_truth\n",
    "    })\n",
    "\n",
    "# Save results\n",
    "pd.DataFrame(results).to_csv(\"simpleqa_initial_outputs.csv\", index=False)\n",
    "print(\"Responses saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5v-dI1d6hTGB"
   },
   "source": [
    "# **SimpleQA Filtering Grader**\n",
    "## LLM Grader 1: SimpleQA Filtering\n",
    "Functions used to triage base model predictions as CORRECT / INCORRECT / NOT_ATTEMPTED.\n",
    "\n",
    "(create_grader_prompt, grade_with_mistral)\n",
    "\n",
    "This grader was used to filter the 500 SimpleQA examples down to the 260 incorrect cases.\n",
    "It assigns one of three labels: `CORRECT`, `INCORRECT`, or `NOT_ATTEMPTED`, following a rubric-driven template.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Configuration for Mistral API\n",
    "# ----------------------------\n",
    "#MISTRAL_API_KEY = \"your_mistral_api_key\"  # Replace with your actual API key\n",
    "GRADER_MODEL = \"mistral-small-latest\"      # Replace if needed\n",
    "TEMPERATURE = 0.0                          # Use 0 for deterministic output\n",
    "\n",
    "# Initialize the Mistral client\n",
    "client = Mistral(api_key=MISTRAL_API_KEY)\n",
    "\n",
    "# ----------------------------\n",
    "# Grader Template\n",
    "# ----------------------------\n",
    "GRADER_TEMPLATE = \"\"\"\n",
    "You are a factuality grader. Given a question, a gold target (correct answer), and a predicted answer, assign one of the following grades:\n",
    "- \"CORRECT\" (fully contains correct information, no contradictions)\n",
    "- \"INCORRECT\" (contains incorrect, misleading, or contradictory information)\n",
    "- \"NOT_ATTEMPTED\" (fails to answer or provides irrelevant information)\n",
    "\n",
    "### Example Grades:\n",
    "CORRECT:\n",
    "- Question: \"Who discovered gravity?\"\n",
    "  - Gold target: \"Isaac Newton\"\n",
    "  - Predicted: \"Isaac Newton\"\n",
    "  - Predicted: \"Newton discovered gravity.\"\n",
    "  - Predicted: \"Most believe Isaac Newton discovered gravity.\"\n",
    "\n",
    "INCORRECT:\n",
    "- Question: \"Who discovered gravity?\"\n",
    "  - Gold target: \"Isaac Newton\"\n",
    "  - Predicted: \"Albert Einstein\"\n",
    "  - Predicted: \"Newton, Einstein, and Galileo discovered gravity.\" (Contradicts correct answer)\n",
    "  - Predicted: \"Gravity was first theorized in 1905.\" (Incorrect)\n",
    "\n",
    "NOT_ATTEMPTED:\n",
    "- Question: \"Who discovered gravity?\"\n",
    "  - Gold target: \"Isaac Newton\"\n",
    "  - Predicted: \"I don't know.\"\n",
    "  - Predicted: \"It depends on interpretation.\"\n",
    "  - Predicted: \"Without more context, I cannot say.\"\n",
    "\n",
    "### Rules:\n",
    "- Ignore capitalization, punctuation, and grammar differences.\n",
    "- Allow hedging (e.g., \"I think\" or \"probably\") as long as the correct answer is included.\n",
    "- If a predicted answer partially matches but is missing key details, grade as INCORRECT.\n",
    "- If a number is the correct answer, it must be accurate to the last significant figure.\n",
    "- Do not punish minor typos in names if the intent is clear.\n",
    "\n",
    "### Now, grade this:\n",
    "Question: {question}\n",
    "Gold target: {target}\n",
    "Predicted answer: {predicted_answer}\n",
    "\n",
    "Reply with:\n",
    "A: CORRECT\n",
    "B: INCORRECT\n",
    "C: NOT_ATTEMPTED\n",
    "\n",
    "Only return \"A\", \"B\", or \"C\" with no extra text.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Helper Functions\n",
    "# ----------------------------\n",
    "def create_grader_prompt(question: str, target: str, predicted_answer: str) -> str:\n",
    "    \"\"\"Formats the grader prompt with the provided question, target, and predicted answer.\"\"\"\n",
    "    return GRADER_TEMPLATE.format(\n",
    "        question=question,\n",
    "        target=target,\n",
    "        predicted_answer=predicted_answer\n",
    "    )\n",
    "\n",
    "def grade_with_mistral(question: str, target: str, predicted_answer: str, retries=3, base_delay=2) -> str:\n",
    "    \"\"\"\n",
    "    Sends the grader prompt to Mistral and extracts the grade letter (A, B, or C).\n",
    "    Implements retry logic with exponential backoff in case of rate limits.\n",
    "\n",
    "    - retries: Number of retry attempts before failing.\n",
    "    - base_delay: Base wait time before retrying (in seconds).\n",
    "    \"\"\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            prompt = create_grader_prompt(question, target, predicted_answer)\n",
    "\n",
    "            # Prepare chat messages\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are a grading assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "\n",
    "            # Call Mistral's API\n",
    "            chat_response = client.chat.complete(\n",
    "                model=GRADER_MODEL,\n",
    "                temperature=TEMPERATURE,\n",
    "                messages=messages\n",
    "            )\n",
    "\n",
    "            # Extract grade from response\n",
    "            output = chat_response.choices[0].message.content\n",
    "            match = re.search(r\"(A|B|C)\", output)\n",
    "            return match.group(0) if match else \"C\"  # Default to \"NOT_ATTEMPTED\"\n",
    "\n",
    "        except Exception as e:\n",
    "            if \"Requests rate limit exceeded\" in str(e):\n",
    "                wait_time = base_delay * (2 ** attempt) + (attempt * 0.5)  # Exponential backoff\n",
    "                print(f\"Rate limit hit. Retrying in {wait_time:.2f} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(f\"Error during API call: {e}\")\n",
    "                return \"C\"  # Default to \"NOT_ATTEMPTED\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NbuNgHeZspVI"
   },
   "source": [
    "# **Reference Enrichment**\n",
    "\n",
    "For each failure case, we prompt the base model with both the question and its ground-truth answer to elicit a complete, natural response—often including an explanation. These enriched outputs serve as the reference samples from which we later extract steering activations. Metadata, including the explanation (if present), is saved for analysis and reproducibility.\n",
    "\n",
    "**Note:** This step may take some time. Precomputed outputs are already included and used for evaluation, so re-execution is not necessary. This cell was intentionally not run to save time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# Corrected Response Generation & Metadata Capture\n",
    "# ----------------------------------------\n",
    "\n",
    "# File paths\n",
    "output_dir = \"output_data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "output_csv = os.path.join(output_dir, \"corrected_responses.csv\")\n",
    "output_json = os.path.join(output_dir, \"corrected_metadata.json\")\n",
    "\n",
    "# Prompt template for natural, guided response\n",
    "CORRECT_RESPONSE_TEMPLATE = \"\"\"\n",
    "Question: {question}\n",
    "Answer: {target}\n",
    "\"\"\".strip()\n",
    "\n",
    "def create_correct_response_prompt(question: str, target: str) -> str:\n",
    "    return CORRECT_RESPONSE_TEMPLATE.format(question=question, target=target)\n",
    "\n",
    "# Load prompts where base model failed\n",
    "df_incorrect = pd.read_csv(\"incorrect.csv\")  # columns: prompt, response, ground_truth\n",
    "\n",
    "# Init output containers\n",
    "corrected_metadata = []\n",
    "corrected_csv_rows = []\n",
    "save_every = 10\n",
    "\n",
    "# Process each prompt\n",
    "for idx, row in df_incorrect.iterrows():\n",
    "    question = row[\"prompt\"]\n",
    "    ground_truth = row[\"ground_truth\"]\n",
    "\n",
    "    prompt_text = create_correct_response_prompt(question, ground_truth)\n",
    "    input_ids = tokenizer(prompt_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "    # Generate full response (answer + explanation)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(input_ids=input_ids, max_new_tokens=100)\n",
    "        corrected_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Split explanation if present\n",
    "    if \"**Explanation:**\" in corrected_response:\n",
    "        qa_text, explanation_text = corrected_response.split(\"**Explanation:**\", 1)\n",
    "        qa_text = qa_text.strip()\n",
    "        explanation_text = explanation_text.strip()\n",
    "    else:\n",
    "        qa_text = corrected_response.strip()\n",
    "        explanation_text = \"\"\n",
    "\n",
    "    # Save metadata\n",
    "    corrected_metadata.append({\n",
    "        \"prompt\": question,\n",
    "        \"ground_truth\": ground_truth,\n",
    "        \"corrected_response\": qa_text,\n",
    "        \"explanation\": explanation_text\n",
    "    })\n",
    "\n",
    "    # Save to CSV row\n",
    "    csv_row = {\n",
    "        \"prompt\": question,\n",
    "        \"ground_truth\": ground_truth,\n",
    "        \"corrected_response\": qa_text,\n",
    "        \"explanation\": explanation_text\n",
    "    }\n",
    "    corrected_csv_rows.append(csv_row)\n",
    "\n",
    "    # Periodic save\n",
    "    if (idx + 1) % save_every == 0:\n",
    "        with open(output_json, \"w\") as f:\n",
    "            json.dump(corrected_metadata, f)\n",
    "        pd.DataFrame(corrected_csv_rows).to_csv(output_csv, index=False)\n",
    "        print(f\"Checkpoint saved at row {idx + 1}\")\n",
    "\n",
    "# Final save\n",
    "with open(output_json, \"w\") as f:\n",
    "    json.dump(corrected_metadata, f)\n",
    "pd.DataFrame(corrected_csv_rows).to_csv(output_csv, index=False)\n",
    "print(\"Final Save Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mj-f67LOnc9l"
   },
   "source": [
    "# **Reference Activation Extraction**\n",
    "\n",
    "This section extracts reference activation vectors for each prompt-layer pair using the enriched prompt (question + ground truth + explanation). We store one averaged vector per layer for each prompt. Vectors are periodically checkpointed for fault tolerance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_reference_vector(prompt, gt, exp, model, tokenizer, layer_id):\n",
    "    full_prompt = f\"Question: {prompt}\\nAnswer and explanation:\\n{gt}\\n{exp}\"\n",
    "    encoded = tokenizer(full_prompt, return_offsets_mapping=True, return_tensors=\"pt\", add_special_tokens=True)\n",
    "    input_ids = encoded[\"input_ids\"].to(\"cuda\")\n",
    "    offsets = encoded[\"offset_mapping\"][0]  # [seq_len, 2]\n",
    "\n",
    "    gt_start = full_prompt.find(gt)\n",
    "    gt_end = gt_start + len(gt) + len(exp)\n",
    "\n",
    "    # Get token indices covering the GT+explanation span\n",
    "    token_indices = [\n",
    "        i for i, (start, end) in enumerate(offsets)\n",
    "        if start >= gt_start and end <= gt_end\n",
    "    ]\n",
    "    if not token_indices:\n",
    "        raise ValueError(\"Could not align ground-truth+explanation in token offsets.\")\n",
    "\n",
    "    captured = {}\n",
    "\n",
    "    def capture_hook(module, _, output):\n",
    "        acts = output[0] if isinstance(output, tuple) else output\n",
    "        captured[\"acts\"] = acts.detach().squeeze(0).to(torch.float32).to(\"cuda\")\n",
    "\n",
    "    handle = model.model.layers[layer_id].register_forward_hook(capture_hook)\n",
    "    _ = model(input_ids=input_ids)\n",
    "    handle.remove()\n",
    "\n",
    "    acts = captured[\"acts\"]\n",
    "    ref_vector = acts[token_indices].mean(dim=0)  # [hidden_dim]\n",
    "    return ref_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"corrected_responses.csv\")\n",
    "layers_to_steer = list(range(0, 26))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** This step may take some time. Precomputed outputs are already included and used for evaluation, so re-execution is not necessary. This cell was intentionally not run to save time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_vectors = {}\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    prompt = row[\"prompt\"]\n",
    "    gt = row[\"ground_truth\"]\n",
    "    exp = row[\"explanation\"] if pd.notna(row[\"explanation\"]) else \"\"\n",
    "\n",
    "    for layer in layers_to_steer:\n",
    "        ref_vec = extract_reference_vector(prompt, gt, exp, model, tokenizer, layer)\n",
    "        reference_vectors[(i, layer)] = ref_vec\n",
    "\n",
    "    if i % 5 == 0:\n",
    "        torch.save(reference_vectors, \"reference_vectors_partial.pt\")\n",
    "        print(f\"Partial save at prompt {i} — {time.strftime('%H:%M:%S')}\")\n",
    "\n",
    "torch.save(reference_vectors, \"reference_vectors.pt\")\n",
    "print(\"Final reference vectors saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E1Jw82CGAhB9"
   },
   "source": [
    "# **Token Overlap and Perplexity for Per-Prompt Optimization**\n",
    "This section defines the token overlap and perplexity metrics used as part of the multi-objective scoring function for Optuna-based hyperparameter optimization. While both metrics are used to guide the search for optimal steering parameters (alpha, gamma), only token overlap is later combined with LLM-based rubric scores for final evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_overlap_score(ground_truth, predicted):\n",
    "    gt_tokens = set(word_tokenize(ground_truth.lower()))\n",
    "    gen_tokens = set(word_tokenize(predicted.lower()))\n",
    "\n",
    "    # Remove punctuation\n",
    "    gt_tokens = {t for t in gt_tokens if t not in string.punctuation}\n",
    "    gen_tokens = {t for t in gen_tokens if t not in string.punctuation}\n",
    "\n",
    "    if not gt_tokens:\n",
    "        return 0.0  # Avoid division by zero\n",
    "\n",
    "    overlap = gt_tokens & gen_tokens\n",
    "    score = len(overlap) / len(gt_tokens)  # Continuous value in [0, 1]\n",
    "    return round(score, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perplexity_score(text):\n",
    "    tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(\"cuda\")\n",
    "    encodings = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        max_length = model.config.n_positions\n",
    "        stride = 512\n",
    "        nlls = []\n",
    "        for i in range(0, encodings.input_ids.size(1), stride):\n",
    "            input_ids = encodings.input_ids[:, i:i+stride]\n",
    "            target_ids = input_ids.clone()\n",
    "            outputs = model(input_ids, labels=target_ids)\n",
    "            neg_log_likelihood = outputs.loss\n",
    "            nlls.append(neg_log_likelihood)\n",
    "\n",
    "        ppl = torch.exp(torch.stack(nlls).mean()).item()\n",
    "    return ppl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qkNABngDCwHm"
   },
   "source": [
    "# **Full-Layer Steering**\n",
    "### Full-Layer Steering Evaluation Logic\n",
    "This section defines the vector injection mechanism across all transformer layers using uniform α and γ parameters. Steering is applied per layer using reference activations, and output quality is evaluated using token overlap and perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_vectors = torch.load(\"reference_vectors.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_steering_hook(ref_vector, alpha=1.0, gamma=0.5):\n",
    "    def hook(module, _, output):\n",
    "        acts = output[0] if isinstance(output, tuple) else output\n",
    "        reshaped = acts.squeeze(0).to(torch.float32)\n",
    "\n",
    "        mu_steered = reshaped.mean(dim=0, keepdim=True)\n",
    "        mu_ref = ref_vector.unsqueeze(0).to(torch.float32)\n",
    "        shift = gamma * (mu_ref - mu_steered)\n",
    "\n",
    "        steered = reshaped + shift\n",
    "        blended = alpha * steered + (1 - alpha) * reshaped\n",
    "        blended = blended.unsqueeze(0).to(dtype=torch.float16, device=acts.device)\n",
    "\n",
    "        return (blended,) + output[1:] if isinstance(output, tuple) else blended\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_single_prompt(prompt_idx, alpha, gamma, tokenizer, model, reference_vectors):\n",
    "    prompt = df[\"prompt\"].iloc[prompt_idx]\n",
    "    gt = df[\"ground_truth\"].iloc[prompt_idx]\n",
    "    exp = df[\"explanation\"].iloc[prompt_idx] if pd.notna(df[\"explanation\"].iloc[prompt_idx]) else \"\"\n",
    "\n",
    "    input_text = f\"Question: {prompt}\\nAnswer and explanation:\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    handles = []\n",
    "    try:\n",
    "        for layer in layers_to_steer:\n",
    "            ref_vec = reference_vectors.get((prompt_idx, layer))\n",
    "            if ref_vec is not None:\n",
    "                hook = make_steering_hook(ref_vec, alpha, gamma)\n",
    "                handle = model.model.layers[layer].register_forward_hook(hook)\n",
    "                handles.append(handle)\n",
    "\n",
    "        output = model.generate(**inputs, max_new_tokens=100, use_cache=False)\n",
    "        decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    finally:\n",
    "        for h in handles:\n",
    "            h.remove()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    token_overlap = get_token_overlap_score(gt, decoded)\n",
    "    perplexity = get_perplexity_score(decoded)\n",
    "\n",
    "    return token_overlap, perplexity, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"full-layer_trials\"\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EcMdKBYLFB8k"
   },
   "source": [
    "**Note:** The cell below runs full prompt-by-prompt Optuna tuning across 260 prompts (each 5 trials). This step may take some time. Precomputed outputs are already included and used for evaluation, so re-execution is not necessary. This cell was intentionally not run to save time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_weight = 0.7  # How much to favor token overlap over fluency\n",
    "summary_path = os.path.join(save_dir, \"per_prompt_optuna_summary_full.csv\")\n",
    "summary_rows = []\n",
    "layers_to_steer = list(range(26))  # Layers 0 to 25\n",
    "\n",
    "if os.path.exists(summary_path):\n",
    "    summary_rows = pd.read_csv(summary_path).to_dict(orient=\"records\")\n",
    "\n",
    "for prompt_idx in range(0, 260):\n",
    "    save_path = os.path.join(save_dir, f\"prompt_{prompt_idx}_full.csv\")\n",
    "    if os.path.exists(save_path):\n",
    "        print(f\"Skipping prompt #{prompt_idx} (already processed)\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Optimizing for prompt #{prompt_idx}\")\n",
    "    trial_logs = []\n",
    "\n",
    "    def objective(trial):\n",
    "        alpha = trial.suggest_float(\"alpha\", 0.01, 1.0)\n",
    "        gamma = trial.suggest_float(\"gamma\", 0.01, 1.0)\n",
    "\n",
    "        token_overlap, perplexity, output_text = evaluate_single_prompt(\n",
    "            prompt_idx=prompt_idx,\n",
    "            alpha=alpha,\n",
    "            gamma=gamma,\n",
    "            tokenizer=tokenizer,\n",
    "            model=model,\n",
    "            reference_vectors=reference_vectors\n",
    "        )\n",
    "\n",
    "        normed_perplexity = min(perplexity / 100, 1.0)\n",
    "        combined_score = lambda_weight * token_overlap - (1 - lambda_weight) * normed_perplexity\n",
    "\n",
    "        trial_logs.append({\n",
    "            \"prompt_idx\": prompt_idx,\n",
    "            \"alpha\": alpha,\n",
    "            \"gamma\": gamma,\n",
    "            \"token_overlap\": token_overlap,\n",
    "            \"perplexity\": perplexity,\n",
    "            \"combined_score\": combined_score,\n",
    "            \"generated_text\": output_text,\n",
    "            \"prompt\": df[\"prompt\"].iloc[prompt_idx],\n",
    "            \"ground_truth\": df[\"ground_truth\"].iloc[prompt_idx],\n",
    "            \"explanation\": df[\"explanation\"].iloc[prompt_idx] if pd.notna(df[\"explanation\"].iloc[prompt_idx]) else \"\"\n",
    "        })\n",
    "\n",
    "        pd.DataFrame(trial_logs).to_csv(save_path, index=False)\n",
    "        return -combined_score\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=5)\n",
    "\n",
    "    best_trial = max(trial_logs, key=lambda x: x[\"combined_score\"])\n",
    "    print(f\"Best score for prompt #{prompt_idx}: {best_trial['combined_score']:.3f}\")\n",
    "    print(best_trial[\"generated_text\"])\n",
    "\n",
    "    summary_rows.append(best_trial)\n",
    "    pd.DataFrame(summary_rows).to_csv(summary_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLkA8pJBoebD"
   },
   "source": [
    "# **Segmented Steering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segmented Steering\n",
    "\n",
    "layer_groups = {\n",
    "    \"early\": list(range(0, 9)),\n",
    "    \"middle\": list(range(9, 18)),\n",
    "    \"late\": list(range(18, 26)),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_single_prompt_segmented(prompt_idx, group_params, tokenizer, model, reference_vectors):\n",
    "    prompt = df[\"prompt\"].iloc[prompt_idx]\n",
    "    gt = df[\"ground_truth\"].iloc[prompt_idx]\n",
    "    exp = df[\"explanation\"].iloc[prompt_idx] if pd.notna(df[\"explanation\"].iloc[prompt_idx]) else \"\"\n",
    "\n",
    "    input_text = f\"Question: {prompt}\\nAnswer and explanation:\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    handles = []\n",
    "    try:\n",
    "        for group_name, layers in layer_groups.items():\n",
    "            alpha = group_params[f'alpha_{group_name}']\n",
    "            gamma = group_params[f'gamma_{group_name}']\n",
    "\n",
    "            for layer in layers:\n",
    "                ref_vec = reference_vectors.get((prompt_idx, layer))\n",
    "                if ref_vec is not None:\n",
    "                    hook = make_steering_hook(ref_vec, alpha, gamma)\n",
    "                    handle = model.model.layers[layer].register_forward_hook(hook)\n",
    "                    handles.append(handle)\n",
    "\n",
    "        output = model.generate(**inputs, max_new_tokens=100, use_cache=False)\n",
    "        decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    finally:\n",
    "        for h in handles:\n",
    "            h.remove()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    token_overlap = get_token_overlap_score(gt, decoded)\n",
    "    perplexity = get_perplexity_score(decoded)\n",
    "\n",
    "    return token_overlap, perplexity, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"segmented_trials\"\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The cell below runs full prompt-by-prompt Optuna tuning across 260 prompts (each 5 trials). This step may take some time. Precomputed outputs are already included and used for evaluation, so re-execution is not necessary. This cell was intentionally not run to save time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track all best trials together\n",
    "summary_path = os.path.join(save_dir, \"per_prompt_optuna_summary_segmented.csv\")\n",
    "summary_rows = []\n",
    "\n",
    "# Load previously saved summary if exists\n",
    "if os.path.exists(summary_path):\n",
    "    summary_rows = pd.read_csv(summary_path).to_dict(orient=\"records\")\n",
    "\n",
    "for prompt_idx in range(0, 260):  # Resume from any index\n",
    "    save_path = os.path.join(save_dir, f\"prompt_{prompt_idx}_all_trials_segmented.csv\")\n",
    "\n",
    "    # Skip if trials for this prompt already exist\n",
    "    if os.path.exists(save_path):\n",
    "        print(f\"Skipping prompt #{prompt_idx} (already processed)\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nOptimizing for prompt #{prompt_idx}\")\n",
    "    trial_logs = []\n",
    "\n",
    "    def objective(trial):\n",
    "        group_params = {\n",
    "            f'alpha_{g}': trial.suggest_float(f'alpha_{g}', 0.01, 1.0)\n",
    "            for g in layer_groups\n",
    "        }\n",
    "        group_params.update({\n",
    "            f'gamma_{g}': trial.suggest_float(f'gamma_{g}', 0.01, 1.0)\n",
    "            for g in layer_groups\n",
    "        })\n",
    "\n",
    "        token_overlap, perplexity, output_text = evaluate_single_prompt_segmented(\n",
    "            prompt_idx=prompt_idx,\n",
    "            group_params=group_params,\n",
    "            tokenizer=tokenizer,\n",
    "            model=model,\n",
    "            reference_vectors=reference_vectors\n",
    "        )\n",
    "\n",
    "        # Normalize perplexity (optional, basic log scale)\n",
    "        normed_perplexity = min(perplexity / 100, 1.0)\n",
    "\n",
    "        # Combine both scores\n",
    "        combined_score = lambda_weight * token_overlap - (1 - lambda_weight) * normed_perplexity\n",
    "\n",
    "        trial_logs.append({\n",
    "            'prompt_idx': prompt_idx,\n",
    "            **group_params,\n",
    "            'token_overlap': token_overlap,\n",
    "            'perplexity': perplexity,\n",
    "            'combined_score': combined_score,\n",
    "            'generated_text': output_text,\n",
    "            'prompt': df[\"prompt\"].iloc[prompt_idx],\n",
    "            'ground_truth': df[\"ground_truth\"].iloc[prompt_idx],\n",
    "            'explanation': df[\"explanation\"].iloc[prompt_idx] if pd.notna(df[\"explanation\"].iloc[prompt_idx]) else \"\"\n",
    "        })\n",
    "\n",
    "        pd.DataFrame(trial_logs).to_csv(save_path, index=False)\n",
    "        return -combined_score  # Minimizing the negative\n",
    "\n",
    "    # Run optimization\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=5)\n",
    "\n",
    "    # Best trial based on combined score\n",
    "    best_trial = max(trial_logs, key=lambda x: x['combined_score'])\n",
    "    print(f\"Best score for prompt #{prompt_idx}: {best_trial['combined_score']:.3f}\")\n",
    "    print(best_trial['generated_text'])\n",
    "\n",
    "    summary_rows.append(best_trial)\n",
    "    pd.DataFrame(summary_rows).to_csv(summary_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DV1vjIAnh6LZ"
   },
   "source": [
    "# **Post-Steering Quality Grader**\n",
    "## LLM Grader 2: Post-Steering Quality Evaluation\n",
    "Functions used to assign 1–5 rubric scores with justification notes.\n",
    "\n",
    "(create_grader_prompt, grade_with_mistral, parse_grader_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Grader Template\n",
    "# ----------------------------\n",
    "GRADER_TEMPLATE = \"\"\"\n",
    "You are a strict but fair evaluator. Evaluate the generated answer for correctness, relevance, and completeness.\n",
    "\n",
    "Question: {question}\n",
    "Generated Answer: {generated_text}\n",
    "Ground Truth Answer: {ground_truth}\n",
    "\n",
    "Give a score from 1 to 5 based on the following:\n",
    "- 5 = Fully correct, complete, and relevant.\n",
    "- 4 = Mostly correct, with minor inaccuracies or missing details.\n",
    "- 3 = Partially correct, has significant omissions or minor errors.\n",
    "- 2 = Mostly incorrect or irrelevant, with small fragments that match.\n",
    "- 1 = Totally wrong, irrelevant, or hallucinated.\n",
    "\n",
    "Also specify:\n",
    "- Hallucination (Y/N): Did the answer include fabricated information not found in the ground truth?\n",
    "- Notes: Brief justification.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grader_prompt(question, ground_truth, generated_text):\n",
    "    return GRADER_TEMPLATE.format(\n",
    "        question=question,\n",
    "        ground_truth=ground_truth,\n",
    "        generated_text=generated_text\n",
    "    )\n",
    "\n",
    "def parse_grader_output(output):\n",
    "    try:\n",
    "        score_match = re.search(r\"Score\\s*[:\\-]?\\s*(\\d)\", output)\n",
    "        hallucination_match = re.search(r\"Hallucination\\s*[:\\-]?\\s*(Y|N)\", output, re.IGNORECASE)\n",
    "        notes_match = re.search(r\"Notes\\s*[:\\-]?\\s*(.*)\", output, re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "        return {\n",
    "            \"score\": int(score_match.group(1)) if score_match else None,\n",
    "            \"hallucination\": hallucination_match.group(1).strip().upper() == \"Y\" if hallucination_match else None,\n",
    "            \"notes\": notes_match.group(1).strip() if notes_match else \"No notes found.\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"score\": None, \"hallucination\": None, \"notes\": f\"Parse error: {str(e)}\"}\n",
    "\n",
    "def grade_with_mistral(question, target, predicted_answer, retries=3, base_delay=2):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are a grading assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": create_grader_prompt(question, target, predicted_answer)}\n",
    "            ]\n",
    "            response = client.chat.complete(\n",
    "                model=GRADER_MODEL,\n",
    "                temperature=TEMPERATURE,\n",
    "                messages=messages\n",
    "            )\n",
    "            output = response.choices[0].message.content.strip()\n",
    "            return parse_grader_output(output)\n",
    "        except Exception as e:\n",
    "            print(f\"API call failed (attempt {attempt+1}): {e}\")\n",
    "            time.sleep(base_delay * (2 ** attempt))\n",
    "    return {\"score\": None, \"hallucination\": None, \"notes\": \"Failed after retries\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YjYvaf2eHe2h"
   },
   "source": [
    "# **Merging Trial Outputs with LLM Grader Results**\n",
    "\n",
    "This section merges generation outputs with LLM-based rubric scores and computes the final evaluation metric (`combined_score`), which averages token overlap and the 1–5 Mistral score. Merging is done here from raw results. Final merged CSVs are not included in the repo, but the raw results are included and the merged versions can be easily regenerated using this code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "syqItjYdIs0H"
   },
   "source": [
    "### Baseline Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "input_csv_path = \"incorrect.csv\"\n",
    "grader_csv_path = \"mistral_grader_results_baseline.csv\"\n",
    "merged_csv_path = \"mistral_grader_results_baseline_with_score.csv\"\n",
    "\n",
    "# Load input and graded results\n",
    "df_input = pd.read_csv(input_csv_path)\n",
    "df_grades = pd.read_csv(grader_csv_path)\n",
    "\n",
    "# Merge using prompt index and generated text\n",
    "merged = pd.merge(\n",
    "    df_input,\n",
    "    df_grades[[\"prompt_id\", \"predicted\", \"score\", \"notes\"]],\n",
    "    left_index=True,\n",
    "    right_on=\"prompt_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Sanity check\n",
    "matched = merged[\"score\"].notna().sum()\n",
    "total = len(merged)\n",
    "print(f\"{matched}/{total} rows matched with Mistral grading scores.\")\n",
    "\n",
    "# Save merged results\n",
    "merged.to_csv(merged_csv_path, index=False)\n",
    "print(f\"Merged and saved to: {merged_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the merged file\n",
    "merged_csv_path = \"mistral_grader_results_baseline_with_score.csv\"\n",
    "df = pd.read_csv(merged_csv_path)\n",
    "\n",
    "# Compute token overlap for each row\n",
    "df[\"token_overlap\"] = df.apply(\n",
    "    lambda row: get_token_overlap_score(row[\"ground_truth\"], row[\"predicted\"]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Compute combined score (handling missing values)\n",
    "df[\"score\"] = pd.to_numeric(df[\"score\"], errors=\"coerce\")\n",
    "df[\"combined_score\"] = 0.5 * df[\"token_overlap\"] + 0.5 * (df[\"score\"] / 5)\n",
    "\n",
    "# Save updated file\n",
    "output_path = \"mistral_grader_results_baseline_with_score_and_overlap.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"Token overlap and combined score added. Saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WlaheBSBIx7O"
   },
   "source": [
    "### Segmented Steering Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# File Paths\n",
    "# ----------------------------\n",
    "merged_path = \"mistral_grader_results_segmented_with_score.csv\"\n",
    "optuna_path = \"per_prompt_optuna_summary_segmented.csv\"\n",
    "grades_path = \"mistral_grader_results_segmented.csv\"\n",
    "\n",
    "# ----------------------------\n",
    "# Load Files\n",
    "# ----------------------------\n",
    "seg_optuna = pd.read_csv(optuna_path)\n",
    "seg_grades = pd.read_csv(grades_path)\n",
    "\n",
    "# Align column names\n",
    "seg_grades = seg_grades.rename(columns={\"prompt_id\": \"prompt_idx\", \"predicted\": \"generated_text\"})\n",
    "\n",
    "# ----------------------------\n",
    "# Merge (this will likely create score_x and score_y)\n",
    "# ----------------------------\n",
    "new_merged = pd.merge(\n",
    "    seg_optuna,\n",
    "    seg_grades,  # full grading file — may include 'score'\n",
    "    on=[\"prompt_idx\", \"generated_text\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# Use the right score column (score_y is from grader)\n",
    "# ----------------------------\n",
    "if \"score_y\" in new_merged.columns:\n",
    "    new_merged[\"score\"] = new_merged[\"score_y\"]\n",
    "    new_merged.drop(columns=[\"score_x\", \"score_y\"], inplace=True)\n",
    "elif \"score\" not in new_merged.columns:\n",
    "    raise ValueError(\"No 'score' column found after merge.\")\n",
    "\n",
    "# ----------------------------\n",
    "# Ensure numeric, handle missing scores\n",
    "# ----------------------------\n",
    "new_merged[\"score\"] = pd.to_numeric(new_merged[\"score\"], errors=\"coerce\").fillna(0)\n",
    "\n",
    "# ----------------------------\n",
    "# Compute combined score\n",
    "# ----------------------------\n",
    "if \"token_overlap\" not in new_merged.columns:\n",
    "    raise ValueError(\"'token_overlap' is missing from the Optuna data — can't compute combined score.\")\n",
    "\n",
    "new_merged[\"combined_score\"] = 0.5 * new_merged[\"token_overlap\"] + 0.5 * (new_merged[\"score\"] / 5)\n",
    "\n",
    "# ----------------------------\n",
    "# Load and append to previous file\n",
    "# ----------------------------\n",
    "if os.path.exists(merged_path):\n",
    "    old_merged = pd.read_csv(merged_path)\n",
    "    key_cols = [\"prompt_idx\", \"generated_text\"]\n",
    "\n",
    "    merged = pd.concat([old_merged, new_merged], ignore_index=True)\n",
    "    merged = merged.drop_duplicates(subset=key_cols, keep=\"last\")\n",
    "else:\n",
    "    merged = new_merged\n",
    "\n",
    "# ----------------------------\n",
    "# Save result\n",
    "# ----------------------------\n",
    "merged.to_csv(merged_path, index=False)\n",
    "print(f\"Merged and saved with updated combined scores to: {merged_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w60ehn_MI9If"
   },
   "source": [
    "### Full-Layer Steering Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load full-layer files\n",
    "full_overlap_df = pd.read_csv(\"per_prompt_optuna_summary_full.csv\")\n",
    "full_llm_df = pd.read_csv(\"mistral_grader_results_full.csv\")\n",
    "\n",
    "# Rename to disambiguate before merge\n",
    "full_overlap_df = full_overlap_df.rename(columns={\"score\": \"token_overlap\"})\n",
    "full_llm_df = full_llm_df.rename(columns={\"prompt_id\": \"prompt_idx\", \"score\": \"llm_score\"})\n",
    "\n",
    "# Merge on prompt index\n",
    "full_merged = pd.merge(full_overlap_df, full_llm_df[[\"prompt_idx\", \"llm_score\"]], on=\"prompt_idx\", how=\"left\")\n",
    "\n",
    "# Rename to match segmented format\n",
    "full_merged = full_merged.rename(columns={\"llm_score\": \"score\"})\n",
    "full_merged[\"method\"] = \"Full-layer\"\n",
    "\n",
    "# Save this aligned version\n",
    "full_merged.to_csv(\"mistral_grader_results_full_merged.csv\", index=False)\n",
    "print(\"Full-layer merged file saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4nVwGNzJG-1D"
   },
   "source": [
    "# **Final Evaluation Data Assembly**\n",
    "\n",
    "This block loads the outputs from all methods (Baseline, Full-layer Steering, and Segmented Steering), computes a combined score (mean of token overlap and LLM-graded score), and prepares the merged dataset used for all performance comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results from baseline, full-layer, and segmented steering methods\n",
    "seg_combined = pd.read_csv(\"mistral_grader_results_segmented_with_score.csv\")\n",
    "full_combined = pd.read_csv(\"mistral_grader_results_full_merged.csv\")\n",
    "base_combined = pd.read_csv(\"mistral_grader_results_baseline_with_score_and_overlap.csv\")\n",
    "\n",
    "# Label each method for downstream analysis\n",
    "seg_combined[\"method\"] = \"Segmented\"\n",
    "full_combined[\"method\"] = \"Full-layer\"\n",
    "base_combined[\"method\"] = \"Baseline\"\n",
    "\n",
    "# Ensure numeric scores are valid\n",
    "for df in [seg_combined, full_combined, base_combined]:\n",
    "    df[\"token_overlap\"] = pd.to_numeric(df[\"token_overlap\"], errors=\"coerce\")\n",
    "    df[\"score\"] = pd.to_numeric(df[\"score\"], errors=\"coerce\")\n",
    "\n",
    "# Compute combined score: 0.5 * token overlap + 0.5 * normalized LLM score\n",
    "def compute_combined(row):\n",
    "    return 0.5 * row[\"token_overlap\"] + 0.5 * (row[\"score\"] / 5)\n",
    "\n",
    "for df in [seg_combined, full_combined, base_combined]:\n",
    "    df[\"combined_score\"] = df.apply(compute_combined, axis=1)\n",
    "\n",
    "# Keep only best-performing trial per prompt\n",
    "seg_combined = seg_combined.sort_values(\"combined_score\", ascending=False).drop_duplicates(\"prompt_idx\")\n",
    "full_combined = full_combined.sort_values(\"combined_score\", ascending=False).drop_duplicates(\"prompt_idx\")\n",
    "base_combined = base_combined.sort_values(\"combined_score\", ascending=False).drop_duplicates(\"prompt_id\")\n",
    "\n",
    "# Combine all methods into one DataFrame for final comparison\n",
    "all_combined = pd.concat([seg_combined, full_combined, base_combined], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set larger global font sizes\n",
    "plt.rcParams.update({\n",
    "    'axes.titlesize': 24,\n",
    "    'axes.labelsize': 20,\n",
    "    'xtick.labelsize': 14,\n",
    "    'ytick.labelsize': 14,\n",
    "    'font.weight': 'bold',\n",
    "    'axes.labelweight': 'bold',\n",
    "    'axes.titleweight': 'bold',\n",
    "})\n",
    "\n",
    "# Create summary manually since baseline has 0 correct\n",
    "data = {\n",
    "    \"method\": [\"Baseline\", \"Full-layer\", \"Segmented\"],\n",
    "    \"correct\": [0, 24, 34],\n",
    "    \"total\": [260, 260, 260]\n",
    "}\n",
    "df_summary = pd.DataFrame(data)\n",
    "df_summary[\"accuracy (%)\"] = df_summary[\"correct\"] / df_summary[\"total\"] * 100\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.grid(True, axis='y', linestyle='-', alpha=0.7)\n",
    "barplot = sns.barplot(data=df_summary, x=\"method\", y=\"accuracy (%)\", palette=\"Set2\")\n",
    "\n",
    "# Annotate each bar\n",
    "for i, row in df_summary.iterrows():\n",
    "    count_text = f\"{row['correct']} / {row['total']}\"\n",
    "    acc_text = f\"{row['accuracy (%)']:.1f}%\"\n",
    "    barplot.text(i, row[\"accuracy (%)\"] + 1.5, f\"{acc_text}\\n({count_text})\",\n",
    "                 ha='center', va='bottom', fontsize=16, weight='bold')\n",
    "\n",
    "# Final touches\n",
    "plt.title(\"% CORRECT (SimpleQA Rubric)\")\n",
    "plt.ylabel(\"Correct Completions (%)\")\n",
    "plt.ylim(0, 20)  # Lower max to make differences visually sharper\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save and show\n",
    "plt.savefig(\"simpleqa_accuracy_barplot.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set larger global font sizes\n",
    "plt.rcParams.update({\n",
    "    'axes.titlesize': 24,\n",
    "    'axes.labelsize': 20,\n",
    "    'xtick.labelsize': 18,\n",
    "    'ytick.labelsize': 18,\n",
    "    'font.weight': 'bold',\n",
    "    'axes.labelweight': 'bold',\n",
    "    'axes.titleweight': 'bold',\n",
    "})\n",
    "\n",
    "# Calculate accuracy (≥ 0.6)\n",
    "all_combined[\"accurate\"] = all_combined[\"combined_score\"].apply(lambda x: int(x >= 0.6))\n",
    "acc_summary = all_combined.groupby(\"method\")[\"accurate\"].agg([\"mean\", \"count\", \"sum\"]).reset_index()\n",
    "acc_summary[\"accuracy (%)\"] = acc_summary[\"mean\"] * 100\n",
    "\n",
    "# Compute % improvement over baseline\n",
    "baseline_acc = acc_summary.loc[acc_summary[\"method\"] == \"Baseline\", \"accuracy (%)\"].values[0]\n",
    "seg_acc = acc_summary.loc[acc_summary[\"method\"] == \"Segmented\", \"accuracy (%)\"].values[0]\n",
    "full_acc = acc_summary.loc[acc_summary[\"method\"] == \"Full-layer\", \"accuracy (%)\"].values[0]\n",
    "\n",
    "seg_improvement = (seg_acc - baseline_acc) / baseline_acc * 100\n",
    "full_improvement = (full_acc - baseline_acc) / baseline_acc * 100\n",
    "\n",
    "print(f\"Segmented improved over Baseline by: {seg_improvement:.2f}%\")\n",
    "print(f\"Full-layer improved over Baseline by: {full_improvement:.2f}%\")\n",
    "\n",
    "# Barplot with counts & labels\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.grid(True, axis='y', linestyle='-', alpha=0.7)\n",
    "barplot = sns.barplot(data=acc_summary, x=\"method\", y=\"accuracy (%)\", palette=\"Set2\")\n",
    "\n",
    "# Annotate bars with count of accurate cases\n",
    "for i, row in acc_summary.iterrows():\n",
    "    count_text = f\"{int(row['sum'])} / {int(row['count'])}\"\n",
    "    acc_text = f\"{row['accuracy (%)']:.1f}%\"\n",
    "    barplot.text(i, row[\"accuracy (%)\"] + 2, f\"{acc_text}\\n({count_text})\",\n",
    "                 ha='center', va='bottom', fontsize=16)\n",
    "\n",
    "plt.title(\"Accuracy: % of Combined Score ≥ 0.6\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.ylim(0, 110)\n",
    "plt.tight_layout()\n",
    "plt.grid(True, axis='y', linestyle='-', alpha=0.7)\n",
    "\n",
    "# Save barplot\n",
    "plt.savefig(\"accuracy_barplot.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match color scheme from the bar chart\n",
    "custom_palette = {\n",
    "    \"Baseline\": \"#66c2a5\",     # green\n",
    "    \"Full-layer\": \"#fc8d62\",   # orange\n",
    "    \"Segmented\": \"#8da0cb\"     # blue/purple\n",
    "}\n",
    "\n",
    "# Set global font sizes\n",
    "plt.rcParams.update({\n",
    "    'axes.titlesize': 24,\n",
    "    'axes.labelsize': 20,\n",
    "    'xtick.labelsize': 18,\n",
    "    'ytick.labelsize': 18,\n",
    "})\n",
    "\n",
    "# Define order explicitly\n",
    "method_order = [\"Baseline\", \"Full-layer\", \"Segmented\"]\n",
    "\n",
    "# Compute stats\n",
    "grouped = all_combined.groupby(\"method\")[\"combined_score\"]\n",
    "medians = grouped.median().reindex(method_order)\n",
    "means = grouped.mean().reindex(method_order)\n",
    "stds = grouped.std().reindex(method_order)\n",
    "whiskers = grouped.max().reindex(method_order)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(9, 6))\n",
    "ax = sns.boxplot(\n",
    "    data=all_combined,\n",
    "    x=\"method\",\n",
    "    y=\"combined_score\",\n",
    "    order=method_order,\n",
    "    palette=custom_palette\n",
    ")\n",
    "\n",
    "plt.title(\"Combined Score Distribution by Method\")\n",
    "plt.ylabel(\"Combined Score (0–1)\")\n",
    "plt.xlabel(\"Method\")\n",
    "plt.grid(True, axis='y', linestyle='-', alpha=0.7)\n",
    "\n",
    "# Annotate stats just below the top whisker\n",
    "for i, method in enumerate(method_order):\n",
    "    median_val = medians[method]\n",
    "    mean_val = means[method]\n",
    "    std_val = stds[method]\n",
    "    top_y = whiskers[method] - 0.05\n",
    "    label = (\n",
    "        f\"Median: {median_val:.2f}\\n\"\n",
    "        f\"Mean: {mean_val:.2f}\\n\"\n",
    "        f\"Std: {std_val:.2f}\"\n",
    "    )\n",
    "    plt.text(\n",
    "        i, top_y, label,\n",
    "        ha='center', va='top',\n",
    "        fontsize=16, fontweight='bold'\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"combined_score_boxplot.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match color scheme\n",
    "custom_palette = {\n",
    "    \"Baseline\": \"#66c2a5\",     # green\n",
    "    \"Full-layer\": \"#fc8d62\",   # orange\n",
    "    \"Segmented\": \"#8da0cb\"     # blue/purple\n",
    "}\n",
    "\n",
    "method_order = [\"Baseline\", \"Full-layer\", \"Segmented\"]\n",
    "\n",
    "# Set global font sizes\n",
    "plt.rcParams.update({\n",
    "    'axes.titlesize': 24,\n",
    "    'axes.labelsize': 20,\n",
    "    'xtick.labelsize': 18,\n",
    "    'ytick.labelsize': 18,\n",
    "})\n",
    "\n",
    "# Calculate summary stats\n",
    "summary_stats = (\n",
    "    all_combined.groupby(\"method\")[\"token_overlap\"]\n",
    "    .agg([\"mean\", \"median\", \"std\"])\n",
    "    .loc[method_order]\n",
    ")\n",
    "\n",
    "# Whisker top values for annotation placement\n",
    "whiskers = all_combined.groupby(\"method\")[\"token_overlap\"].max().loc[method_order]\n",
    "\n",
    "# Plot boxplot\n",
    "plt.figure(figsize=(9, 6))\n",
    "ax = sns.boxplot(\n",
    "    data=all_combined,\n",
    "    x=\"method\",\n",
    "    y=\"token_overlap\",\n",
    "    order=method_order,\n",
    "    palette=custom_palette\n",
    ")\n",
    "\n",
    "# Annotate stats just below whiskers\n",
    "for i, method in enumerate(method_order):\n",
    "    median_val = summary_stats.loc[method, \"median\"]\n",
    "    mean_val = summary_stats.loc[method, \"mean\"]\n",
    "    std_val = summary_stats.loc[method, \"std\"]\n",
    "    top_y = whiskers[method] - 0.05\n",
    "\n",
    "    label = (\n",
    "        f\"Median: {median_val:.2f}\\n\"\n",
    "        f\"Mean: {mean_val:.2f}\\n\"\n",
    "        f\"Std: {std_val:.2f}\"\n",
    "    )\n",
    "    plt.text(\n",
    "        i, top_y, label,\n",
    "        ha='center', va='top',\n",
    "        fontsize=16, fontweight='bold'\n",
    "    )\n",
    "\n",
    "plt.title(\"Token Score Distribution by Method\")\n",
    "plt.ylabel(\"Token Overlap Score\")\n",
    "plt.xlabel(\"Steering Method\")\n",
    "plt.grid(True, axis='y', linestyle='-', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"token_overlap_boxplot.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark examples with token overlap ≥ 0.5 as accurate\n",
    "all_combined[\"accurate\"] = all_combined[\"token_overlap\"].apply(lambda x: int(x >= 0.5))\n",
    "\n",
    "# Compute accuracy summary\n",
    "acc_summary = all_combined.groupby(\"method\").agg(\n",
    "    correct=(\"accurate\", \"sum\"),\n",
    "    total=(\"accurate\", \"count\")\n",
    ").reset_index()\n",
    "acc_summary[\"accuracy (%)\"] = acc_summary[\"correct\"] / acc_summary[\"total\"] * 100\n",
    "\n",
    "# Global font config\n",
    "plt.rcParams.update({\n",
    "    'axes.titlesize': 24,\n",
    "    'axes.titleweight': 'bold',\n",
    "    'axes.labelsize': 20,\n",
    "    'axes.labelweight': 'bold',\n",
    "    'xtick.labelsize': 18,\n",
    "    'ytick.labelsize': 18,\n",
    "    'xtick.color': 'black',\n",
    "    'ytick.color': 'black',\n",
    "    'font.weight': 'bold',\n",
    "    'legend.fontsize': 16,\n",
    "    'legend.title_fontsize': 18\n",
    "})\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(9, 6))\n",
    "ax = sns.barplot(data=acc_summary, x=\"method\", y=\"accuracy (%)\", palette=\"Set2\")\n",
    "\n",
    "# Annotate bars\n",
    "for i, row in acc_summary.iterrows():\n",
    "    label = f\"{row['accuracy (%)']:.1f}%\\n({int(row['correct'])} / {int(row['total'])})\"\n",
    "    ax.text(i, row[\"accuracy (%)\"] + 2, label, ha=\"center\", va=\"bottom\", fontsize=16, fontweight='bold')\n",
    "\n",
    "# Axis labels and title\n",
    "plt.title(\"Accuracy: % Token Overlap ≥ 0.5\", fontsize=24, fontweight='bold')\n",
    "plt.ylabel(\"Accuracy (%)\", fontsize=20, fontweight='bold')\n",
    "plt.xlabel(\"Method\", fontsize=20, fontweight='bold')\n",
    "\n",
    "# Ticks\n",
    "ax.set_xticklabels(ax.get_xticklabels(), fontsize=18, fontweight='bold')\n",
    "ax.set_yticklabels(ax.get_yticks(), fontsize=18, fontweight='bold')\n",
    "\n",
    "plt.ylim(0, 110)\n",
    "plt.grid(axis='y', linestyle='-', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save\n",
    "plt.savefig(\"token_overlap_accuracy_barplot.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set aesthetics\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "\n",
    "# Load all CSVs\n",
    "baseline_df = pd.read_csv(\"mistral_grader_results_baseline_with_score_and_overlap.csv\")\n",
    "full_grades = pd.read_csv(\"mistral_grader_results_full_merged.csv\")\n",
    "seg_df = pd.read_csv(\"mistral_grader_results_segmented.csv\")\n",
    "\n",
    "# Add method labels\n",
    "baseline_df[\"method\"] = \"Baseline\"\n",
    "full_grades[\"method\"] = \"Full-layer Steering\"\n",
    "seg_df[\"method\"] = \"Segmented Steering\"\n",
    "\n",
    "# Rename for consistency\n",
    "baseline_df = baseline_df.rename(columns={\"prompt_id\": \"prompt_idx\"})\n",
    "seg_df = seg_df.rename(columns={\"prompt_id\": \"prompt_idx\"})\n",
    "full_grades = full_grades.rename(columns={\"prompt_idx\": \"prompt_idx\"})  # already good\n",
    "\n",
    "# Keep only necessary columns\n",
    "baseline_df = baseline_df[[\"prompt_idx\", \"score\", \"method\"]]\n",
    "full_grades = full_grades[[\"prompt_idx\", \"score\", \"method\"]]\n",
    "seg_df = seg_df[[\"prompt_idx\", \"score\", \"method\"]]\n",
    "\n",
    "# Combine\n",
    "combined_df = pd.concat([baseline_df, full_grades, seg_df], ignore_index=True)\n",
    "\n",
    "# Keep only best score per prompt per method\n",
    "combined_df = combined_df.sort_values(\"score\", ascending=False).drop_duplicates(subset=[\"prompt_idx\", \"method\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define consistent custom colors\n",
    "custom_palette = {\n",
    "    \"Baseline\": \"#66c2a5\",             # green\n",
    "    \"Full-layer Steering\": \"#fc8d62\",  # orange\n",
    "    \"Segmented Steering\": \"#8da0cb\"    # blue/purple\n",
    "}\n",
    "\n",
    "# Set font sizes globally\n",
    "plt.rcParams.update({\n",
    "    'axes.titlesize': 24,\n",
    "    'axes.labelsize': 20,\n",
    "    'xtick.labelsize': 18,\n",
    "    'ytick.labelsize': 18,\n",
    "    'legend.fontsize': 16,\n",
    "    'legend.title_fontsize': 18\n",
    "})\n",
    "\n",
    "# Set figure size and style\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create the countplot\n",
    "ax = sns.countplot(\n",
    "    data=combined_df,\n",
    "    x=\"score\",\n",
    "    hue=\"method\",\n",
    "    palette=custom_palette,\n",
    "    hue_order=[\"Baseline\", \"Full-layer Steering\", \"Segmented Steering\"]\n",
    ")\n",
    "\n",
    "# Add count labels on top of each bar\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt='%d', label_type='edge', padding=3, fontsize=14, fontweight='bold')\n",
    "\n",
    "# Final formatting\n",
    "plt.title(\"LLM Score Distribution by Method\", fontsize=24, fontweight='bold')\n",
    "plt.xlabel(\"LLM Grade (1–5)\", fontsize=20, fontweight='bold')\n",
    "plt.ylabel(\"Count\", fontsize=20, fontweight='bold')\n",
    "plt.xticks(fontsize=16, fontweight='bold')\n",
    "plt.yticks(fontsize=16, fontweight='bold')\n",
    "\n",
    "legend = plt.legend(title=\"Method\", fontsize=14, title_fontsize=14)\n",
    "for text in legend.get_texts():\n",
    "    text.set_fontweight('bold')\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(\"llm_score_countplot.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize(score):\n",
    "    return int(score >= 4)\n",
    "\n",
    "acc_df = combined_df.copy()\n",
    "acc_df[\"is_correct\"] = acc_df[\"score\"].apply(binarize)\n",
    "\n",
    "# Compute accuracy + counts\n",
    "llm_acc = acc_df.groupby(\"method\").agg(\n",
    "    correct=(\"is_correct\", \"sum\"),\n",
    "    total=(\"is_correct\", \"count\")\n",
    ").reset_index()\n",
    "llm_acc[\"accuracy (%)\"] = llm_acc[\"correct\"] / llm_acc[\"total\"] * 100\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))  # Slightly reduced height\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "ax = sns.barplot(data=llm_acc, x=\"method\", y=\"accuracy (%)\", palette=\"Set2\")\n",
    "\n",
    "# Add % and count annotations just above the bar\n",
    "for i, row in llm_acc.iterrows():\n",
    "    label = f\"{row['accuracy (%)']:.1f}%\\n({int(row['correct'])} / {int(row['total'])})\"\n",
    "    ax.text(\n",
    "        i, row[\"accuracy (%)\"] + 0.5,\n",
    "        label,\n",
    "        ha=\"center\", va=\"bottom\",\n",
    "        fontsize=14, fontweight='bold'\n",
    "    )\n",
    "\n",
    "# Bold styling and font control\n",
    "plt.title(\"Accuracy: % of Responses with LLM Score ≥ 4\", fontsize=24, fontweight='bold', pad=10)\n",
    "plt.ylabel(\"Accuracy (%)\", fontsize=18, fontweight='bold', labelpad=8)\n",
    "plt.xlabel(\"Method\", fontsize=18, fontweight='bold')\n",
    "plt.xticks(fontsize=14, fontweight='bold')\n",
    "plt.yticks(fontsize=14, fontweight='bold')\n",
    "\n",
    "# Tighter y-axis upper bound\n",
    "plt.ylim(0, max(llm_acc[\"accuracy (%)\"]) + 8)\n",
    "\n",
    "plt.grid(axis='y', linestyle='-', alpha=0.7)\n",
    "plt.tight_layout(pad=0.5)\n",
    "\n",
    "# Save to file\n",
    "plt.savefig(\"llm_accuracy_barplot.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
